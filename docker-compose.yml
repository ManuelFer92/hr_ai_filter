# HR AI Filter - Docker Compose
# Run: docker compose up -d

services:
  # ============================================
  # PostgreSQL - Core Data + MLflow Backend
  # ============================================
  postgres:
    image: postgres:15-alpine
    container_name: hr_postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-hr_user}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-hr_password}
      POSTGRES_DB: ${POSTGRES_DB:-hr_ai_filter}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-hr_user} -d ${POSTGRES_DB:-hr_ai_filter}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ============================================
  # pgAdmin - PostgreSQL Web UI
  # Login: admin@example.com / admin
  # ============================================
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: hr_pgadmin
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_SERVER_MODE: "False"
    ports:
      - "5050:80"
    depends_on:
      - postgres
    restart: unless-stopped

  # ============================================
  # ChromaDB - Vector Store for Embeddings
  # Note: Chroma uses port 8000 internally
  # ============================================
  chroma:
    image: chromadb/chroma:latest
    container_name: hr_chroma
    environment:
      ANONYMIZED_TELEMETRY: "false"
    volumes:
      - chroma_data:/chroma/chroma
    ports:
      - "8585:8000"
    # Healthcheck disabled - use service_started condition
    restart: unless-stopped

  # ============================================
  # MLflow - Experiment Tracking
  # Using SQLite backend for simplicity
  # ============================================
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.12.1
    container_name: hr_mlflow
    environment:
      MLFLOW_BACKEND_STORE_URI: sqlite:///mlflow/mlflow.db
      MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlflow/artifacts
    volumes:
      - mlflow_artifacts:/mlflow
    ports:
      - "5000:5000"
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:///mlflow/mlflow.db
      --default-artifact-root /mlflow/artifacts
    restart: unless-stopped

  # ============================================
  # Ollama - Local LLM (Llama 3.1)
  # Auto-pulls model on first startup
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: hr_ollama
    ports:
      - "11435:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_MODEL=${LLM_MODEL:-llama3.1:8b}
    # Pull model after Ollama starts (runs in background)
    entrypoint: ["/bin/sh", "-c", "ollama serve & sleep 5 && ollama pull ${OLLAMA_MODEL:-llama3.1:8b} && wait"]
    healthcheck:
      test: ["CMD-SHELL", "ollama list | grep -q llama || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 120s
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  # ============================================
  # Backend - FastAPI
  # ============================================
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: hr_backend
    environment:
      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-hr_user}:${POSTGRES_PASSWORD:-hr_password}@postgres:5432/${POSTGRES_DB:-hr_ai_filter}
      # LLM
      OLLAMA_HOST: http://ollama:11434
      GOOGLE_API_KEY: ${GOOGLE_API_KEY:-}
      LLM_PROVIDER: ${LLM_PROVIDER:-ollama}
      LLM_MODEL: ${LLM_MODEL:-llama3.1:8b}
      # MLflow
      MLFLOW_TRACKING_URI: http://mlflow:5000
      # Chroma
      CHROMA_HOST: chroma
      CHROMA_PORT: 8585
      # General
      PYTHONUNBUFFERED: 1
    volumes:
      - ./hr_ai_filter:/app/hr_ai_filter:delegated
      - ./data:/app/data:delegated
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_healthy
      mlflow:
        condition: service_started
      chroma:
        condition: service_started
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ============================================
  # Frontend - Streamlit
  # ============================================
  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: hr_frontend
    environment:
      API_URL: http://backend:8000
      PYTHONUNBUFFERED: 1
    volumes:
      - ./hr_ai_filter/frontend:/app/hr_ai_filter/frontend:delegated
    ports:
      - "8501:8501"
    depends_on:
      - backend
    restart: unless-stopped

volumes:
  postgres_data:
  chroma_data:
  mlflow_artifacts:
  ollama_data:

networks:
  default:
    name: hr_ai_network
