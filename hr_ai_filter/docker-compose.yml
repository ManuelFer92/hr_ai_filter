version: "3.9"

services:

  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.12.1
    ports:
      - "5000:5000"
    command: mlflow ui --host 0.0.0.0
    volumes:
      - ./mlruns:/mlruns

  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - mlflow
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - OLLAMA_BASE_URL=http://host.docker.internal:11434/api/generate

  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    depends_on:
      - backend
